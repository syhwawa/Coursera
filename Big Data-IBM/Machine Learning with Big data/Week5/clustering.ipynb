{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Regression, Cluster Analysis, and Association Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from notebook import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.load('C:/Users/leicheng/Downloads/minute_weather.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Subset and remove unused data. Let's count the number of rows in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1587257"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 1.5 million rows in the DataFrame. Clustering this data on your computer in the Cloudera VM can take a long time, so let's only one-tenth of the data. We can subset by calling filter() and using the rowID column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158726"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterDF = df.filter((df.rowID % 10) == 0)\n",
    "filterDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the summary statistics using describe():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rowID</th>\n",
       "      <td>158726</td>\n",
       "      <td>793625.0</td>\n",
       "      <td>458203.9375103623</td>\n",
       "      <td>0</td>\n",
       "      <td>1587250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hpwren_timestamp</th>\n",
       "      <td>158726</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-09-10 00:00:49</td>\n",
       "      <td>2014-09-10 23:53:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air_pressure</th>\n",
       "      <td>158726</td>\n",
       "      <td>916.8301614102414</td>\n",
       "      <td>3.0517165528314516</td>\n",
       "      <td>905.0</td>\n",
       "      <td>929.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air_temp</th>\n",
       "      <td>158726</td>\n",
       "      <td>61.8515891536364</td>\n",
       "      <td>11.833569210641642</td>\n",
       "      <td>31.64</td>\n",
       "      <td>99.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_wind_direction</th>\n",
       "      <td>158680</td>\n",
       "      <td>162.15610032770354</td>\n",
       "      <td>95.27820101905921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_wind_speed</th>\n",
       "      <td>158680</td>\n",
       "      <td>2.7752148979077447</td>\n",
       "      <td>2.0576239697426435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_wind_direction</th>\n",
       "      <td>158680</td>\n",
       "      <td>163.46214393748426</td>\n",
       "      <td>92.45213853838722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_wind_speed</th>\n",
       "      <td>158680</td>\n",
       "      <td>3.4005577262415194</td>\n",
       "      <td>2.418801620809888</td>\n",
       "      <td>0.1</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_wind_direction</th>\n",
       "      <td>158680</td>\n",
       "      <td>166.77401688933702</td>\n",
       "      <td>97.44110914784571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_wind_speed</th>\n",
       "      <td>158680</td>\n",
       "      <td>2.134664103856878</td>\n",
       "      <td>1.742112505242437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain_accumulation</th>\n",
       "      <td>158725</td>\n",
       "      <td>3.178453299732825E-4</td>\n",
       "      <td>0.01123597908603982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain_duration</th>\n",
       "      <td>158725</td>\n",
       "      <td>0.4096267128681682</td>\n",
       "      <td>8.665522693479774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative_humidity</th>\n",
       "      <td>158726</td>\n",
       "      <td>47.609469778108185</td>\n",
       "      <td>26.214408535062045</td>\n",
       "      <td>0.9</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                     1                    2  \\\n",
       "summary              count                  mean               stddev   \n",
       "rowID               158726              793625.0    458203.9375103623   \n",
       "hpwren_timestamp    158726                  None                 None   \n",
       "air_pressure        158726     916.8301614102414   3.0517165528314516   \n",
       "air_temp            158726      61.8515891536364   11.833569210641642   \n",
       "avg_wind_direction  158680    162.15610032770354    95.27820101905921   \n",
       "avg_wind_speed      158680    2.7752148979077447   2.0576239697426435   \n",
       "max_wind_direction  158680    163.46214393748426    92.45213853838722   \n",
       "max_wind_speed      158680    3.4005577262415194    2.418801620809888   \n",
       "min_wind_direction  158680    166.77401688933702    97.44110914784571   \n",
       "min_wind_speed      158680     2.134664103856878    1.742112505242437   \n",
       "rain_accumulation   158725  3.178453299732825E-4  0.01123597908603982   \n",
       "rain_duration       158725    0.4096267128681682    8.665522693479774   \n",
       "relative_humidity   158726    47.609469778108185   26.214408535062045   \n",
       "\n",
       "                                      3                    4  \n",
       "summary                             min                  max  \n",
       "rowID                                 0              1587250  \n",
       "hpwren_timestamp    2011-09-10 00:00:49  2014-09-10 23:53:29  \n",
       "air_pressure                      905.0                929.5  \n",
       "air_temp                          31.64                 99.5  \n",
       "avg_wind_direction                  0.0                359.0  \n",
       "avg_wind_speed                      0.0                 31.9  \n",
       "max_wind_direction                  0.0                359.0  \n",
       "max_wind_speed                      0.1                 36.0  \n",
       "min_wind_direction                  0.0                359.0  \n",
       "min_wind_speed                      0.0                 31.6  \n",
       "rain_accumulation                   0.0                 3.12  \n",
       "rain_duration                       0.0               2960.0  \n",
       "relative_humidity                   0.9                 93.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterDF.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather measurements in this dataset were collected during a drought in San Diego. We can count the how many values of rain accumulation and duration are 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157812"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterDF.filter(filterDF.rain_accumulation == 0.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157237"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterDF.filter(filterDF.rain_duration == 0.0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most the values for these columns are 0, let's drop them from the DataFrame to speed up our analyses. We can also drop the hpwren_timestamp column since we do not use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingDF = filterDF.drop(\"rain_accumulation\").drop(\"rain_duration\").drop(\"hpwren_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = workingDF.count()\n",
    "workingDF = workingDF.na.drop()\n",
    "after = workingDF.count()\n",
    "before -after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Scale the data. Since the features are on different scales (e.g., air pressure values are in the 900’s, while relative humidities range from 0 to 100), they need to be scaled.  We will scale them so that each feature will have a value of 0 for the mean, and a value of 1 for the standard deviation.\n",
    "/First, we will combine the columns into a single vector column. Let's look at the columns in the DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rowID',\n",
       " 'air_pressure',\n",
       " 'air_temp',\n",
       " 'avg_wind_direction',\n",
       " 'avg_wind_speed',\n",
       " 'max_wind_direction',\n",
       " 'max_wind_speed',\n",
       " 'min_wind_direction',\n",
       " 'min_wind_speed',\n",
       " 'relative_humidity']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workingDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to include rowID since it is the row number. The minimum wind measurements have a high correlation to the average wind measurements, so we will not include them either. Let's create an array of the columns we want to combine, and use VectorAssembler to create the vector column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureUsed = ['air_pressure',\n",
    " 'air_temp',\n",
    " 'avg_wind_direction',\n",
    " 'avg_wind_speed',\n",
    " 'max_wind_direction',\n",
    " 'max_wind_speed',\n",
    " 'relative_humidity' ]\n",
    "assembler = VectorAssembler(inputCols=featureUsed, outputCol=\"feature_unscaled\")\n",
    "assembled = assembler.transform(workingDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"feature_unscaled\", outputCol=\"features\", withStd = True, withMean = True)\n",
    "scalerModel = scaler.fit(assembled)\n",
    "scalerData = scalerModel.transform(assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The withMean argument specifies to center the data with the mean before scaling, and withStd specifies to scale the data to the unit standard deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Create elbow plot. The k-means algorithm requires that the value of k, the number of clusters, to be specified.  To determine a good value for k, we will use the “elbow” method.  This method involves applying k-means, using different values for k, and calculating the within-cluster sum-of-squared error (WSSE).  Since this means applying k-means multiple times, this process can be very compute-intensive.  To speed up the process, we will use only a subset of the dataset.  We will take every third sample from the dataset to create this subset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerData = scalerData.select(\"features\", \"rowID\")\n",
    "\n",
    "elbowset = scalerData.filter((scalerData.rowID % 3) == 0).select(\"features\")\n",
    "elbowset.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line calls the persist() method to tell Spark to keep the data in memory (if possible), which will speed up the computations.\n",
    "\n",
    "Let's compute the k-means clusters for k = 2 to 30 to create an elbow plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'notebook.utils' has no attribute 'elbow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0810fb2b41c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwsseList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melbowset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'notebook.utils' has no attribute 'elbow'"
     ]
    }
   ],
   "source": [
    "clusters = range(2, 31)\n",
    "wsseList = utils.elbow(elbowset, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Cluster using selected k. Let's select the data we want to cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerDataFeat = scalerData.select(\"features\")\n",
    "scalerDataFeat.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans =KMeans(k=12, seed=1)\n",
    "model = kmeans.fit(scalerDataFeat)\n",
    "transformed = model.transform(scalerDataFeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates a new KMeans instance with 12 clusters and a specific seed value. (As in previous hands-on activities, we use a specific seed value for reproducible results.) The second line fits the data to the model, and the third applies the model to the data.\n",
    "\n",
    "Once the model is created, we can determine the center measurement of each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.23862654,  0.31134856,  1.88818402, -0.65263889, -1.5507189 ,\n",
       "        -0.57731762, -0.27628725]),\n",
       " array([-0.77189796, -0.10325633,  0.44012451,  1.53154542,  0.52564327,\n",
       "         1.46055857,  0.22969611]),\n",
       " array([ 1.189134  , -0.25128657, -1.15481277,  2.07893983, -1.05267645,\n",
       "         2.1952909 , -1.13101972]),\n",
       " array([-1.55854953, -1.40903273,  0.39115013,  2.16691259,  0.50010737,\n",
       "         2.13527566,  1.50575845]),\n",
       " array([ 0.53986028, -0.96950459,  0.83538105, -0.55276297,  1.04974319,\n",
       "        -0.52675456,  0.95661038]),\n",
       " array([-0.71820835, -0.00888325,  0.0991225 , -0.65685135,  0.28292777,\n",
       "        -0.66241855,  0.34981034]),\n",
       " array([-0.18319451,  0.85775867, -1.28472876, -0.59754225, -1.13652643,\n",
       "        -0.61274245, -0.62565969]),\n",
       " array([-0.03959381,  0.72903715,  0.39829629,  0.37118846,  0.51223037,\n",
       "         0.32973963, -0.30378681]),\n",
       " array([ 0.13219485, -0.83781548, -1.22239113, -0.5531094 , -1.07278337,\n",
       "        -0.56782597,  0.89822913]),\n",
       " array([ 0.18177807,  0.88247336,  0.87294009, -0.69435152,  1.10067395,\n",
       "        -0.6734131 , -0.78246879]),\n",
       " array([ 1.3691501 , -0.07038314, -1.13129823, -0.13048433, -0.98809452,\n",
       "        -0.11123514, -0.98849907]),\n",
       " array([-0.69024133, -1.24498484,  0.39173733,  0.32645615,  0.48812719,\n",
       "         0.31337108,  1.40994844])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers = model.clusterCenters()\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to compare the cluster centers by just looking at these numbers. So we will use plots in the next step to visualize them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel plots of clusters and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7. Create parallel plots of clusters and analysis. A parallel coordinates plot is a great way to visualize multi-dimensional data. Each line plots the centroid of a cluster, and all of the features are plotted together. Recall that the feature values were scaled to have mean = 0 and standard deviation = 1. So the values on the y-axis of these parallel coordinates plots show the number of standard deviations from the mean.  For example, +1 means one standard deviation higher than the mean of all samples, and -1 means one standard deviation lower than the mean of all samples.\n",
    "\n",
    "We'll create the plots with matplotlib using a Pandas DataFrame each row contains the cluster center coordinates and cluster label. (Matplotlib can plot Pandas DataFrames, but not Spark DataFrames.) Let's use the pd_centers() function in the utils.py library to create the Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'notebook.utils' has no attribute 'pd_centers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-cee36842391a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpd_centers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureUsed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'notebook.utils' has no attribute 'pd_centers'"
     ]
    }
   ],
   "source": [
    "P = utils.pd_centers(featureUsed, centers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
